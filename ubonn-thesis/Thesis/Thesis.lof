\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\babel@toc {UKenglish}{}
\babel@toc {ngerman}{}
\babel@toc {UKenglish}{}
\babel@toc {UKenglish}{}
\babel@toc {UKenglish}{}
\babel@toc {ngerman}{}
\babel@toc {UKenglish}{}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {\relax 2.1}{\ignorespaces Schematic depiction of the SM. The masses of the in the SM massive particles are taken from \cite {PDG2020}. The electroweak force is divided into the electromagnetic force and the weak nuclear force. \cite {SMScheme}\relax }}{3}{figure.caption.3}
\contentsline {figure}{\numberline {\relax 2.2}{\ignorespaces The two different categorizes of Top production\relax }}{5}{figure.caption.4}
\contentsline {figure}{\numberline {\relax 2.3}{\ignorespaces [INTNOTE] Two possible production processes of $t\bar {t}t\bar {t}$ in the SM.\relax }}{6}{figure.caption.5}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {\relax 3.1}{\ignorespaces The accelerator complex of CERN including the LHC. The accelerator chain for the protons is shown as well as the four major experiments.[???]\relax }}{9}{figure.caption.8}
\contentsline {figure}{\numberline {\relax 3.2}{\ignorespaces Illustration of the ATLAS detector and its sub-units.[???]\relax }}{11}{figure.caption.13}
\contentsline {figure}{\numberline {\relax 3.3}{\ignorespaces Interaction of different types of particles with different segments of the ATLAS detector.[???]\relax }}{12}{figure.caption.17}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {\relax 4.1}{\ignorespaces Sum of the transverse momentum of all leptons and jets $H_{\text {T}}$. The dark red line shows the signal scaled to the total background yield.\relax }}{15}{figure.caption.22}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {\relax 5.1}{\ignorespaces Schematic structure of a deep neural network with fully connected layers. The input values of the given task $x_i$ are fed into the neurons (depicted as circles) of the input layer and processed in the hidden layers. The weights which connect neurons located at different layers are represented by a solid arrow. In the output layer, all information is combined and predictions (depicted as dark squares) about the classes are made. (Figure taken from \cite {patterson2017deep})\relax }}{17}{figure.caption.24}
\contentsline {figure}{\numberline {\relax 5.2}{\ignorespaces Schematic description of a Recurrent Neural Network neuron unrolled through time. The information is propagated in a feed-forward manner using the same three weight matrices at each time step.\relax }}{20}{figure.caption.27}
\contentsline {figure}{\numberline {\relax 5.3}{\ignorespaces Three different models (black) to distinguish the red from the blue circles. [???]\relax }}{21}{figure.caption.29}
\contentsline {figure}{\numberline {\relax 5.4}{\ignorespaces Four commonly used activation functions [???]\relax }}{23}{figure.caption.32}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {\relax 6.1}{\ignorespaces Splitting Strategy that is applied to ensure an unbiased estimation of both overtraining and performance [].\relax }}{25}{figure.caption.34}
\contentsline {figure}{\numberline {\relax 6.2}{\ignorespaces Sum of the continuous MV2c10 b-tagging score\relax }}{28}{figure.caption.35}
\contentsline {figure}{\numberline {\relax 6.3}{\ignorespaces Jet multiplicity\relax }}{28}{figure.caption.35}
\contentsline {figure}{\numberline {\relax 6.4}{\ignorespaces minimum angular distance between any b-jet pair\relax }}{28}{figure.caption.35}
\contentsline {figure}{\numberline {\relax 6.5}{\ignorespaces Legend\relax }}{28}{figure.caption.35}
\contentsline {figure}{\numberline {\relax 6.6}{\ignorespaces The difference in classification between a Neural Network trained with and without renormalization event weights.\relax }}{29}{figure.caption.37}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {\relax 7.1}{\ignorespaces The averaged AUCs of FNNs with different number of parameters where the training is stop after 10, 80 and at the best-performing Epoch. The average includes the AUCs of the 4 best-performing Neural Networks of different depth.\relax }}{32}{figure.caption.39}
\contentsline {figure}{\numberline {\relax 7.2}{\ignorespaces The average overtraining for different FNNs configurations. The training is stopped after 10, 20, 80 and at the best-performing Epoch. The average includes the overtrainings of the 4 best-performing Neural Networks of different depth.\relax }}{33}{figure.caption.40}
\contentsline {figure}{\numberline {\relax 7.3}{\ignorespaces The average observed AUC for FNNs with different numbers of hidden layers. The average includes the AUCs of the 4 best-performing Neural Networks of different size.\relax }}{33}{figure.caption.41}
\contentsline {figure}{\numberline {\relax 7.4}{\ignorespaces The overtraining for FNNs in the region were the observed AUCs are close to the obtained best-performance.\relax }}{34}{figure.caption.42}
\contentsline {figure}{\numberline {\relax 7.5}{\ignorespaces Obtained AUCs for different choices of activation functions and weight initialization.\relax }}{35}{figure.caption.44}
\contentsline {figure}{\numberline {\relax 7.6}{\ignorespaces The overtraining for different choices of activation functions and weight initialization.\relax }}{36}{figure.caption.45}
\contentsline {figure}{\numberline {\relax 7.7}{\ignorespaces The obtained average AUC of FNNs with different learning rates and three distinct optimization algorithms. The average includes the AUCs of the 3 best-performing Neural Networks with different batchsizes.\relax }}{37}{figure.caption.47}
\contentsline {figure}{\numberline {\relax 7.8}{\ignorespaces The overtraining of FNNs for different learning rates and three distinct optimization algorithms.\relax }}{38}{figure.caption.48}
\contentsline {figure}{\numberline {\relax 7.9}{\ignorespaces The AUC as a function of the batch size and the learning rate. The Neural Network were trained using SGD.\relax }}{38}{figure.caption.49}
\contentsline {figure}{\numberline {\relax 7.10}{\ignorespaces Training for three different learning rates at a minimum of the Loss function. Illustrating the advantage of polynomial learning rate decay.\relax }}{39}{figure.caption.51}
\contentsline {figure}{\numberline {\relax 7.11}{\ignorespaces Obtained AUCs for FNNs trained using polynomial learning rate decay.\relax }}{40}{figure.caption.52}
\contentsline {figure}{\numberline {\relax 7.12}{\ignorespaces The overtraining for FNNs trained using polynomial learning rate decay.\relax }}{40}{figure.caption.53}
\contentsline {figure}{\numberline {\relax 7.13}{\ignorespaces a) The learning rate schedule for cyclic learning rate decay with and without additional decay of the maximum learning rate. b) The topology of a Loss function for which cyclic learning rate decay can improve the performance of a neural Network.\relax }}{41}{figure.caption.55}
\contentsline {figure}{\numberline {\relax 7.14}{\ignorespaces The dependence between the maximum and the minimum learning rate of FNNs trained with cyclic learning rate decay\relax }}{42}{figure.caption.56}
\contentsline {figure}{\numberline {\relax 7.15}{\ignorespaces The observed AUCs for different maximum learning rate using cyclic decay with and without additional decay.\relax }}{43}{figure.caption.57}
\contentsline {figure}{\numberline {\relax 7.16}{\ignorespaces The dependence of the AUC on the stepsize chosen for the cyclic learning rate decay.\relax }}{43}{figure.caption.58}
\contentsline {figure}{\numberline {\relax 7.17}{\ignorespaces Obtained AUCs for different learning rates and distinct choices of $p$ the Dropout probability.\relax }}{44}{figure.caption.60}
\contentsline {figure}{\numberline {\relax 7.18}{\ignorespaces The dependence of the AUC and the overtraining for different Dropout probabilities and l2 (Ridge) regularization.\relax }}{45}{figure.caption.61}
\contentsline {figure}{\numberline {\relax 7.19}{\ignorespaces Observed Neural Network scores for the Neural Network trained with the background dataset containing even event numbers (a) and on the background dataset constraining odd event numbers (b).\relax }}{45}{figure.caption.63}
\contentsline {figure}{\numberline {\relax 7.20}{\ignorespaces The $H_{\text {T}}$ distributions after applying cut to the FNN scores obtained for the training on the background dataset containing even event numbers (a) and the background dataset constraining odd event numbers (b).\relax }}{46}{figure.caption.64}
\contentsline {figure}{\numberline {\relax 7.21}{\ignorespaces Observed AUCs for the bootstrapping on the validation dataset.\relax }}{47}{figure.caption.65}
\contentsline {figure}{\numberline {\relax 7.22}{\ignorespaces The confusion matrix and the Neural Network score for the multi classification with 14 classes.\relax }}{48}{figure.caption.67}
\contentsline {figure}{\numberline {\relax 7.23}{\ignorespaces The confusion matrix for the multi classification with 14 classes where an additional class weight was applied during the training of the Classifier.\relax }}{49}{figure.caption.69}
\contentsline {figure}{\numberline {\relax 7.24}{\ignorespaces The confusion matrix and the Neural Network score for the multi classification with 3 classes.\relax }}{50}{figure.caption.71}
\contentsline {figure}{\numberline {\relax 7.25}{\ignorespaces The average overtraining for different RNNs configurations. The training is stopped after 20, 40, 60 and after 80 Epochs. The average includes the overtrainings of the 4 best-performing Neural Networks of different depth.\relax }}{51}{figure.caption.73}
\contentsline {figure}{\numberline {\relax 7.26}{\ignorespaces The obtained average AUC of RNNs with different learning rates. The average includes the AUCs of the 2 best-performing Neural Networks with different batchsizes.\relax }}{52}{figure.caption.75}
\contentsline {figure}{\numberline {\relax 7.27}{\ignorespaces The AUC as a function of the batch size and the learning rate.\relax }}{53}{figure.caption.76}
\contentsline {figure}{\numberline {\relax 7.28}{\ignorespaces The $H_{\text {T}}$ distribution after applying a cut to the RNN score obtained for the training on the background dataset containing even event numbers.\relax }}{54}{figure.caption.78}
\contentsline {figure}{\numberline {\relax 7.29}{\ignorespaces Observed AUCs for the bootstrapping on the validation dataset.\relax }}{55}{figure.caption.79}
\contentsline {figure}{\numberline {\relax 7.30}{\ignorespaces Training times per Epoch for FNNs and RNNs of different sizes trained on CPUs and GPUs.\relax }}{56}{figure.caption.80}
\contentsline {figure}{\numberline {\relax 7.31}{\ignorespaces Training times per Epoch for FNNs and RNNs trained with different batch sizes and on CPUs and GPUs.\relax }}{57}{figure.caption.81}
\contentsline {figure}{\numberline {\relax 7.32}{\ignorespaces Mass distributions of the truth matched reconstruction top quarks and W bosons.\relax }}{59}{figure.caption.84}
\contentsline {figure}{\numberline {\relax 7.33}{\ignorespaces Observed reconstruction where the critical values were tuned such that the fraction of events in all three channels fit the true $t\bar {t}t\bar {t}$ distribution.\relax }}{60}{figure.caption.86}
\contentsline {figure}{\numberline {\relax 7.34}{\ignorespaces $\delta R$ between the reconstructed top quarks and the truth top quarks for the $t\bar {t}t\bar {t}$ process (a) and the $t\bar {t}t$ process.\relax }}{61}{figure.caption.87}
\contentsline {figure}{\numberline {\relax 7.35}{\ignorespaces Kinematic distributions for the truth and reconstructed top quarks and W bosons.\relax }}{61}{figure.caption.88}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
