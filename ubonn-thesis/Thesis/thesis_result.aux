\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Results}{31}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:Results}{{7}{31}{Results}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Signal Classification using Feedforward Neural Networks}{31}{section.7.1}}
\newlabel{sec:FNN_results}{{7.1}{31}{Signal Classification using Feedforward Neural Networks}{section.7.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Architecture}{31}{section*.37}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.1}{\ignorespaces The averaged AUCs of FNNs with different number of parameters where the training is stopped after 10, 80 and at the best-performing Epoch. The average includes the AUCs of the 4 best-performing Neural Networks of different depth.\relax }}{32}{figure.caption.38}}
\newlabel{fig:AucPara}{{\relax 7.1}{32}{The averaged AUCs of FNNs with different number of parameters where the training is stopped after 10, 80 and at the best-performing Epoch. The average includes the AUCs of the 4 best-performing Neural Networks of different depth.\relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.2}{\ignorespaces The average overtraining for different FNN configurations. The training is stopped after 10, 20, 80 and at the best-performing Epoch. The average includes the overtrainings of the 4 best-performing Neural Networks of different depth.\relax }}{33}{figure.caption.39}}
\newlabel{fig:OvPara}{{\relax 7.2}{33}{The average overtraining for different FNN configurations. The training is stopped after 10, 20, 80 and at the best-performing Epoch. The average includes the overtrainings of the 4 best-performing Neural Networks of different depth.\relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.3}{\ignorespaces The average AUC for FNNs with different numbers of hidden layers. The average includes the AUCs of the 4 best-performing Neural Networks of different size.\relax }}{33}{figure.caption.40}}
\newlabel{fig:AucLayers}{{\relax 7.3}{33}{The average AUC for FNNs with different numbers of hidden layers. The average includes the AUCs of the 4 best-performing Neural Networks of different size.\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.4}{\ignorespaces The overtraining for FNNs of different sizes and depth, in the region where the observed AUCs are close to the best-performance obtained.\relax }}{34}{figure.caption.41}}
\newlabel{fig:RegionPara}{{\relax 7.4}{34}{The overtraining for FNNs of different sizes and depth, in the region where the observed AUCs are close to the best-performance obtained.\relax }{figure.caption.41}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Weight Initialization and Activation Functions}{35}{section*.42}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.5}{\ignorespaces AUCs obtained for different choices of activation functions and weight initialization.\relax }}{35}{figure.caption.43}}
\newlabel{fig:HeatActiv}{{\relax 7.5}{35}{AUCs obtained for different choices of activation functions and weight initialization.\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.6}{\ignorespaces The overtraining for different choices of activation functions and weight initialization.\relax }}{36}{figure.caption.44}}
\newlabel{fig:OvActiv}{{\relax 7.6}{36}{The overtraining for different choices of activation functions and weight initialization.\relax }{figure.caption.44}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Learning Rate and Optimization Algorithms}{37}{section*.45}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.7}{\ignorespaces The obtained average AUC of FNNs with different learning rates and three distinct optimization algorithms. The average includes the AUCs of the 3 best-performing Neural Networks with different batch sizes.\relax }}{37}{figure.caption.46}}
\newlabel{fig:LrAuc}{{\relax 7.7}{37}{The obtained average AUC of FNNs with different learning rates and three distinct optimization algorithms. The average includes the AUCs of the 3 best-performing Neural Networks with different batch sizes.\relax }{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.8}{\ignorespaces The overtraining of FNNs for different learning rates $Lr$ and three distinct optimization algorithms.\relax }}{38}{figure.caption.47}}
\newlabel{fig:OvLr}{{\relax 7.8}{38}{The overtraining of FNNs for different learning rates $Lr$ and three distinct optimization algorithms.\relax }{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.9}{\ignorespaces The AUC as a function of the batch size and the learning rate. The Neural Networks are trained using SGD.\relax }}{38}{figure.caption.48}}
\newlabel{fig:SGDBatch}{{\relax 7.9}{38}{The AUC as a function of the batch size and the learning rate. The Neural Networks are trained using SGD.\relax }{figure.caption.48}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Polynomial Learning Rate Decay}{39}{section*.49}}
\newlabel{fig:TooLow}{{\relax 7.10(a)}{39}{\relax }{figure.caption.50}{}}
\newlabel{sub@fig:TooLow}{{(a)}{39}{\relax }{figure.caption.50}{}}
\newlabel{fig:JustRight}{{\relax 7.10(b)}{39}{\relax }{figure.caption.50}{}}
\newlabel{sub@fig:JustRight}{{(b)}{39}{\relax }{figure.caption.50}{}}
\newlabel{fig:TooHigh}{{\relax 7.10(c)}{39}{\relax }{figure.caption.50}{}}
\newlabel{sub@fig:TooHigh}{{(c)}{39}{\relax }{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.10}{\ignorespaces Training of a Neural Network trained with a too low (a), a too high and a polynomial decaying learning rate. \cite {lrdecay} \relax }}{39}{figure.caption.50}}
\newlabel{fig:PolyTrain}{{\relax 7.10}{39}{Training of a Neural Network trained with a too low (a), a too high and a polynomial decaying learning rate. \cite {lrdecay} \relax }{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.11}{\ignorespaces Obtained AUCs for FNNs trained using polynomial learning rate decay.\relax }}{40}{figure.caption.51}}
\newlabel{fig:PolyAuc}{{\relax 7.11}{40}{Obtained AUCs for FNNs trained using polynomial learning rate decay.\relax }{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.12}{\ignorespaces The overtraining for FNNs trained using polynomial learning rate decay.\relax }}{40}{figure.caption.52}}
\newlabel{fig:PolyOv}{{\relax 7.12}{40}{The overtraining for FNNs trained using polynomial learning rate decay.\relax }{figure.caption.52}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Cyclic Learning Rate Decay}{41}{section*.53}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.13}{\ignorespaces a) The learning rate schedule for cyclic learning rate decay with and without additional decay of the maximum learning rate ``max\_ lr''. b) The topology of a Loss function for which cyclic learning rate decay can improve the performance of a Neural Network. The flags 1, 2 indicate the position of local minima where as flag 3 indicates the position of the global maximum. \cite {cyl1,cyl2}\relax }}{41}{figure.caption.54}}
\newlabel{fig:CylicTrain}{{\relax 7.13}{41}{a) The learning rate schedule for cyclic learning rate decay with and without additional decay of the maximum learning rate ``max\_ lr''. b) The topology of a Loss function for which cyclic learning rate decay can improve the performance of a Neural Network. The flags 1, 2 indicate the position of local minima where as flag 3 indicates the position of the global maximum. \cite {cyl1,cyl2}\relax }{figure.caption.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.14}{\ignorespaces The dependence between the maximum and the minimum learning rates of FNNs trained with cyclic learning rate decay.\relax }}{42}{figure.caption.55}}
\newlabel{fig:CylicHeatLr}{{\relax 7.14}{42}{The dependence between the maximum and the minimum learning rates of FNNs trained with cyclic learning rate decay.\relax }{figure.caption.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.15}{\ignorespaces The observed AUCs for different maximum learning rate using cyclic decay with and without additional decay.\relax }}{43}{figure.caption.56}}
\newlabel{fig:CylicLr}{{\relax 7.15}{43}{The observed AUCs for different maximum learning rate using cyclic decay with and without additional decay.\relax }{figure.caption.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.16}{\ignorespaces The dependence of the AUC on the stepsize chosen for the cyclic learning rate decay.\relax }}{43}{figure.caption.57}}
\newlabel{fig:CylicStepSize}{{\relax 7.16}{43}{The dependence of the AUC on the stepsize chosen for the cyclic learning rate decay.\relax }{figure.caption.57}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Regularization}{44}{section*.58}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.17}{\ignorespaces Obtained AUCs for different learning rates and distinct choices of $p$, the Dropout probability.\relax }}{44}{figure.caption.59}}
\newlabel{fig:ReguDrop}{{\relax 7.17}{44}{Obtained AUCs for different learning rates and distinct choices of $p$, the Dropout probability.\relax }{figure.caption.59}{}}
\newlabel{fig:ReguLr}{{\relax 7.18(a)}{45}{\relax }{figure.caption.60}{}}
\newlabel{sub@fig:ReguLr}{{(a)}{45}{\relax }{figure.caption.60}{}}
\newlabel{fig:ReguOv}{{\relax 7.18(b)}{45}{\relax }{figure.caption.60}{}}
\newlabel{sub@fig:ReguOv}{{(b)}{45}{\relax }{figure.caption.60}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.18}{\ignorespaces The dependence of the AUC (a) and the overtraining (b) for different Dropout probabilities and l2 (Ridge) regularization.\relax }}{45}{figure.caption.60}}
\newlabel{fig:Regu}{{\relax 7.18}{45}{The dependence of the AUC (a) and the overtraining (b) for different Dropout probabilities and l2 (Ridge) regularization.\relax }{figure.caption.60}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Achieved Separation}{45}{section*.61}}
\newlabel{fig:ScoreEven}{{\relax 7.19(a)}{45}{\relax }{figure.caption.62}{}}
\newlabel{sub@fig:ScoreEven}{{(a)}{45}{\relax }{figure.caption.62}{}}
\newlabel{fig:ScoreOdd}{{\relax 7.19(b)}{45}{\relax }{figure.caption.62}{}}
\newlabel{sub@fig:ScoreOdd}{{(b)}{45}{\relax }{figure.caption.62}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.19}{\ignorespaces Observed Neural Network scores for the Neural Network trained with the background dataset containing even event numbers (a) and on the background dataset constraining odd event numbers (b).\relax }}{45}{figure.caption.62}}
\newlabel{fig:Scores}{{\relax 7.19}{45}{Observed Neural Network scores for the Neural Network trained with the background dataset containing even event numbers (a) and on the background dataset constraining odd event numbers (b).\relax }{figure.caption.62}{}}
\newlabel{fig:HTEven}{{\relax 7.20(a)}{46}{Dataset with even numbered background events\relax }{figure.caption.63}{}}
\newlabel{sub@fig:HTEven}{{(a)}{46}{Dataset with even numbered background events\relax }{figure.caption.63}{}}
\newlabel{fig:HTOdd}{{\relax 7.20(b)}{46}{Dataset with odd numbered background events\relax }{figure.caption.63}{}}
\newlabel{sub@fig:HTOdd}{{(b)}{46}{Dataset with odd numbered background events\relax }{figure.caption.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.20}{\ignorespaces $H_{\text  {T}}$ distributions after applying a cut (optimized on the signal efficiency) to the FNN scores obtained for the training on the background dataset containing even event numbers (a) and the background dataset constraining odd event numbers (b). The last bin additionally contains all events with $H_{\text  {T}} > 2200$.\relax }}{46}{figure.caption.63}}
\newlabel{fig:HTFNN}{{\relax 7.20}{46}{$H_{\text {T}}$ distributions after applying a cut (optimized on the signal efficiency) to the FNN scores obtained for the training on the background dataset containing even event numbers (a) and the background dataset constraining odd event numbers (b). The last bin additionally contains all events with $H_{\text {T}} > 2200$.\relax }{figure.caption.63}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Validation and Error Estimation}{47}{section*.64}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.21}{\ignorespaces AUCs observed by using the bootstrap method on the validation set. The seed quoted (times 365) is the random seed used to generate the different resampled versions of the validation set. The red line indicates the mean of all observed AUCs.\relax }}{47}{figure.caption.65}}
\newlabel{fig:BootStrapFNN}{{\relax 7.21}{47}{AUCs observed by using the bootstrap method on the validation set. The seed quoted (times 365) is the random seed used to generate the different resampled versions of the validation set. The red line indicates the mean of all observed AUCs.\relax }{figure.caption.65}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Multi Classification using Feedforward Neural Networks}{48}{section.7.2}}
\newlabel{sec:Multi}{{7.2}{48}{Multi Classification using Feedforward Neural Networks}{section.7.2}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Fourteen Classes}{48}{section*.66}}
\newlabel{fig:CMunW}{{\relax 7.22(a)}{48}{\relax }{figure.caption.67}{}}
\newlabel{sub@fig:CMunW}{{(a)}{48}{\relax }{figure.caption.67}{}}
\newlabel{fig:Score14}{{\relax 7.22(b)}{48}{\relax }{figure.caption.67}{}}
\newlabel{sub@fig:Score14}{{(b)}{48}{\relax }{figure.caption.67}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.22}{\ignorespaces The confusion matrix (a) and the Neural Network score (b) for the multi classification with 14 classes.\relax }}{48}{figure.caption.67}}
\newlabel{fig:14Class}{{\relax 7.22}{48}{The confusion matrix (a) and the Neural Network score (b) for the multi classification with 14 classes.\relax }{figure.caption.67}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Class Weights}{49}{section*.68}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.23}{\ignorespaces The confusion matrix for the multi classification with 14 classes where an additional class weight was applied during the training of the Classifier.\relax }}{49}{figure.caption.69}}
\newlabel{fig:CMWed}{{\relax 7.23}{49}{The confusion matrix for the multi classification with 14 classes where an additional class weight was applied during the training of the Classifier.\relax }{figure.caption.69}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Three Classes}{50}{section*.70}}
\newlabel{fig:CM3}{{\relax 7.24(a)}{50}{\relax }{figure.caption.71}{}}
\newlabel{sub@fig:CM3}{{(a)}{50}{\relax }{figure.caption.71}{}}
\newlabel{fig:Score3}{{\relax 7.24(b)}{50}{\relax }{figure.caption.71}{}}
\newlabel{sub@fig:Score3}{{(b)}{50}{\relax }{figure.caption.71}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.24}{\ignorespaces The confusion matrix (a) and the Neural Network score (b) for the multi classification with 3 classes.\relax }}{50}{figure.caption.71}}
\newlabel{fig:3Class}{{\relax 7.24}{50}{The confusion matrix (a) and the Neural Network score (b) for the multi classification with 3 classes.\relax }{figure.caption.71}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Signal Classification using Recurrent Neural Networks}{51}{section.7.3}}
\newlabel{sec:RNN_results}{{7.3}{51}{Signal Classification using Recurrent Neural Networks}{section.7.3}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Architecture}{51}{section*.72}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.25}{\ignorespaces The average overtraining for different RNNs configurations. The training is stopped after 20, 40, 60 and after 80 Epochs. The average includes the overtrainings of the 4 best-performing Neural Networks of different depth.\relax }}{51}{figure.caption.73}}
\newlabel{fig:ParaRNN}{{\relax 7.25}{51}{The average overtraining for different RNNs configurations. The training is stopped after 20, 40, 60 and after 80 Epochs. The average includes the overtrainings of the 4 best-performing Neural Networks of different depth.\relax }{figure.caption.73}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Learning Rate and Batch size}{52}{section*.74}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.26}{\ignorespaces The obtained average AUC of RNNs with different learning rates. The average includes the AUCs of the 2 best-performing Neural Networks with different batchsizes.\relax }}{52}{figure.caption.75}}
\newlabel{fig:LrRNN}{{\relax 7.26}{52}{The obtained average AUC of RNNs with different learning rates. The average includes the AUCs of the 2 best-performing Neural Networks with different batchsizes.\relax }{figure.caption.75}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.27}{\ignorespaces The AUC as a function of the batch size and the learning rate.\relax }}{53}{figure.caption.76}}
\newlabel{fig:BatchRNN}{{\relax 7.27}{53}{The AUC as a function of the batch size and the learning rate.\relax }{figure.caption.76}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Achieved Separation}{54}{section*.77}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.28}{\ignorespaces The $H_{\text  {T}}$ distribution after applying a cut (optimized on the signal efficiency) to the RNN score obtained for the training on the background dataset containing even event numbers. The last bin additionally contains all events with $H_{\text  {T}} > 2200$.\relax }}{54}{figure.caption.78}}
\newlabel{fig:RNNHT}{{\relax 7.28}{54}{The $H_{\text {T}}$ distribution after applying a cut (optimized on the signal efficiency) to the RNN score obtained for the training on the background dataset containing even event numbers. The last bin additionally contains all events with $H_{\text {T}} > 2200$.\relax }{figure.caption.78}{}}
\newlabel{eq:kappa}{{\relax 7.3}{54}{Achieved Separation}{equation.7.3.3}{}}
\newlabel{eq:zeta}{{\relax 7.4}{54}{Achieved Separation}{equation.7.3.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {\relax 7.1}{\ignorespaces The reduction measure $\kappa $ (defined in Equation \ref  {eq:kappa}) for the high FNN and RNN region and there scaled difference $\zeta $ (defined in Equation \ref  {eq:zeta}).\relax }}{55}{table.caption.79}}
\newlabel{tab:Rejcomb}{{\relax 7.1}{55}{The reduction measure $\kappa $ (defined in Equation \ref {eq:kappa}) for the high FNN and RNN region and there scaled difference $\zeta $ (defined in Equation \ref {eq:zeta}).\relax }{table.caption.79}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Validation and Error Estimation}{55}{section*.80}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.29}{\ignorespaces AUCs observed by using the bootstrap method on the validation set. The seed quoted (times 365) is the random seed used to generate the different resampled versions of the validation set. The red line indicates the mean of all observed AUCs.\relax }}{55}{figure.caption.81}}
\newlabel{fig:RNNBootStrap}{{\relax 7.29}{55}{AUCs observed by using the bootstrap method on the validation set. The seed quoted (times 365) is the random seed used to generate the different resampled versions of the validation set. The red line indicates the mean of all observed AUCs.\relax }{figure.caption.81}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Performance Evaluation of CPUs and GPUs}{56}{section.7.4}}
\newlabel{sec:CPUGPU}{{7.4}{56}{Performance Evaluation of CPUs and GPUs}{section.7.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.30}{\ignorespaces Training times per Epoch for FNNs and RNNs of different sizes trained on CPUs and GPUs.\relax }}{56}{figure.caption.82}}
\newlabel{fig:CPUGPUPara}{{\relax 7.30}{56}{Training times per Epoch for FNNs and RNNs of different sizes trained on CPUs and GPUs.\relax }{figure.caption.82}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.31}{\ignorespaces Training times per Epoch for FNNs and RNNs trained with different batch sizes and on CPUs and GPUs.\relax }}{57}{figure.caption.83}}
\newlabel{fig:CPUGPUBatch}{{\relax 7.31}{57}{Training times per Epoch for FNNs and RNNs trained with different batch sizes and on CPUs and GPUs.\relax }{figure.caption.83}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Reconstruction of Two Hadronic Top Quarks}{58}{section.7.5}}
\newlabel{sec:TwoHadTop}{{7.5}{58}{Reconstruction of Two Hadronic Top Quarks}{section.7.5}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Reconstruction principle}{58}{section*.84}}
\newlabel{eq:Chi2had}{{\relax 7.5}{58}{Reconstruction principle}{equation.7.5.5}{}}
\newlabel{eq:Chi2}{{\relax 7.6}{58}{Reconstruction principle}{equation.7.5.6}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Calibration}{59}{section*.85}}
\newlabel{fig:Calt}{{\relax 7.32(a)}{59}{Top Quark\relax }{figure.caption.86}{}}
\newlabel{sub@fig:Calt}{{(a)}{59}{Top Quark\relax }{figure.caption.86}{}}
\newlabel{fig:CalW}{{\relax 7.32(b)}{59}{$W$ boson\relax }{figure.caption.86}{}}
\newlabel{sub@fig:CalW}{{(b)}{59}{$W$ boson\relax }{figure.caption.86}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.32}{\ignorespaces Mass distributions of the truth matched and reconstructed top quarks (a) and W bosons (b).\relax }}{59}{figure.caption.86}}
\newlabel{fig:Cal}{{\relax 7.32}{59}{Mass distributions of the truth matched and reconstructed top quarks (a) and W bosons (b).\relax }{figure.caption.86}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Reconstruction Results}{60}{section*.87}}
\newlabel{fig:Reco4t}{{\relax 7.33(a)}{60}{$t\bar {t}t\bar {t}$ events\relax }{figure.caption.88}{}}
\newlabel{sub@fig:Reco4t}{{(a)}{60}{$t\bar {t}t\bar {t}$ events\relax }{figure.caption.88}{}}
\newlabel{fig:Reco3t}{{\relax 7.33(b)}{60}{$t\bar {t}t$ events\relax }{figure.caption.88}{}}
\newlabel{sub@fig:Reco3t}{{(b)}{60}{$t\bar {t}t$ events\relax }{figure.caption.88}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.33}{\ignorespaces Observed reconstruction where the critical values were tuned such that the fraction of events in all three channels fit the true $t\bar {t}t\bar {t}$ distribution.\relax }}{60}{figure.caption.88}}
\newlabel{fig:RecoResult}{{\relax 7.33}{60}{Observed reconstruction where the critical values were tuned such that the fraction of events in all three channels fit the true $t\bar {t}t\bar {t}$ distribution.\relax }{figure.caption.88}{}}
\newlabel{fig:dR3}{{\relax 7.34(a)}{61}{\relax }{figure.caption.89}{}}
\newlabel{sub@fig:dR3}{{(a)}{61}{\relax }{figure.caption.89}{}}
\newlabel{fig:dR4}{{\relax 7.34(b)}{61}{\relax }{figure.caption.89}{}}
\newlabel{sub@fig:dR4}{{(b)}{61}{\relax }{figure.caption.89}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.34}{\ignorespaces $\Delta R$ between the reconstructed top quarks and the truth top quarks for the $t\bar {t}t\bar {t}$ process (a) and the $t\bar {t}t$ process (b). The last bin contains additionally all reconstructed top quarks with $\Delta R > 6$.\relax }}{61}{figure.caption.89}}
\newlabel{fig:dR}{{\relax 7.34}{61}{$\Delta R$ between the reconstructed top quarks and the truth top quarks for the $t\bar {t}t\bar {t}$ process (a) and the $t\bar {t}t$ process (b). The last bin contains additionally all reconstructed top quarks with $\Delta R > 6$.\relax }{figure.caption.89}{}}
\newlabel{fig:pthad1}{{\relax 7.35(a)}{61}{Top $p_{\text {T}}$ for the one hadronic top category\relax }{figure.caption.90}{}}
\newlabel{sub@fig:pthad1}{{(a)}{61}{Top $p_{\text {T}}$ for the one hadronic top category\relax }{figure.caption.90}{}}
\newlabel{fig:pthad2}{{\relax 7.35(b)}{61}{Top $p_{\text {T}}$ for the two hadronic top category\relax }{figure.caption.90}{}}
\newlabel{sub@fig:pthad2}{{(b)}{61}{Top $p_{\text {T}}$ for the two hadronic top category\relax }{figure.caption.90}{}}
\newlabel{fig:etat}{{\relax 7.35(c)}{61}{Top $\eta $ for the one hadronic top category\relax }{figure.caption.90}{}}
\newlabel{sub@fig:etat}{{(c)}{61}{Top $\eta $ for the one hadronic top category\relax }{figure.caption.90}{}}
\newlabel{fig:etaW}{{\relax 7.35(d)}{61}{W boson $\eta $ for the one hadronic top category\relax }{figure.caption.90}{}}
\newlabel{sub@fig:etaW}{{(d)}{61}{W boson $\eta $ for the one hadronic top category\relax }{figure.caption.90}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 7.35}{\ignorespaces Kinematic distributions for the truth and reconstructed top quarks and W bosons. The last bin in all distribution contains additionally the events which fall into bins above the maximum x-axis range.\relax }}{61}{figure.caption.90}}
\newlabel{fig:kinReco}{{\relax 7.35}{61}{Kinematic distributions for the truth and reconstructed top quarks and W bosons. The last bin in all distribution contains additionally the events which fall into bins above the maximum x-axis range.\relax }{figure.caption.90}{}}
\@setckpt{thesis_result}{
\setcounter{page}{62}
\setcounter{equation}{6}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{7}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{35}
\setcounter{table}{1}
\setcounter{parentequation}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{4}
\setcounter{subtable}{0}
\setcounter{r@tfl@t}{0}
\setcounter{PWSTtable}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{9}
\setcounter{bookmark@seq@number}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{89}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{cbx@tempcnta}{0}
\setcounter{cbx@tempcntb}{22}
\setcounter{float@type}{4}
\setcounter{linenumber}{1}
\setcounter{LN@truepage}{67}
\setcounter{section@level}{4}
}
